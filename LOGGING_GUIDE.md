# Doc Matrix - Logging Guide

## Overview

Comprehensive logging has been added to both frontend and backend to track LLM operations, API calls, and execution progress. This helps identify when the system is waiting on LLM responses vs. experiencing errors.

## Backend Logging

### Configuration

Logging is configured in `backend/app/logging_config.py`:
- **Console output**: Real-time logs to terminal
- **File output**: Persistent logs in `backend/logs/doc_matrix.log`
- **Default level**: INFO (can be changed to DEBUG for more detail)

### Log Locations

**File**: `backend/logs/doc_matrix.log`
- All application logs are written here
- Rotates automatically when large
- Survives application restarts

### Backend Log Messages

#### Startup
```
ğŸš€ Doc Matrix starting...
ğŸ“ Current root: /path/to/folder
âœ… Flask app initialized
```

#### LLM Requests (`llm.py`)
```
ğŸš€ Starting LLM request: model=gpt-5.2, max_tokens=4096, temp=0.3
  ğŸ“¡ Attempt 1/3 - Sending request to OpenRouter...
âœ… LLM response received: model=gpt-5.2, tokens=1234, time=5.23s
```

With errors:
```
  âŒ HTTP error 429: Rate limit exceeded
  â³ Rate limited! Waiting 2.0s before retry...
  ğŸ”„ Retrying in 2.0s...
ğŸ’¥ All 3 attempts failed after 8.45s: <error details>
```

#### Execution (`executor.py`)
```
â–¶ï¸  Starting full execution for project: Q4 Analysis
ğŸ“Š Execution plan: 5 documents Ã— 3 questions = 15 cells, mode=parallel, model=gpt-5.2
ğŸ”„ Processing batch 1/2: 10 cells (10/15 total)
âœ“ Cell completed: document1.pdf Ã— col_1
âŒ Cell failed (document2.pdf Ã— What is...): Connection timeout
ğŸ¯ Generating summaries...
âœ… Execution complete for Q4 Analysis: 15/15 cells, 1 errors
```

For individual cells:
```
ğŸ”¹ Executing cell: document1.pdf Ã— col_abc123
âœ… Cell complete: document1.pdf Ã— col_abc123
```

For rows:
```
ğŸ“„ Executing row: document1.pdf
  Processing 3 questions for document1.pdf
```

For columns:
```
ğŸ“Š Executing column: col_abc123
  Processing 5 documents for column col_abc123
```

#### API Endpoints (`api.py`)
```
ğŸ¬ API: Execute all cells - project=Q4 Analysis, model=gpt-5.2
âœ… API: Execute all complete - project=Q4 Analysis
âŒ API: Execute all failed - Project not found
ğŸ’¥ API: Execute all error - <exception details>
```

```
ğŸ”¹ API: Execute cell - project=Q4 Analysis, file=doc1.pdf, column=col_1
âœ… API: Cell complete - doc1.pdf Ã— col_1
```

```
ğŸ’¬ API: Chat query - project=Q4 Analysis, message=What are the key...
âœ… API: Chat response sent - project=Q4 Analysis
```

### Viewing Backend Logs

**Terminal**: Logs appear in real-time where you started the server

**Log File**:
```bash
tail -f backend/logs/doc_matrix.log
```

**Filter specific operations**:
```bash
# Show only LLM requests
grep "ğŸš€ Starting LLM" backend/logs/doc_matrix.log

# Show only errors
grep "âŒ\|ğŸ’¥" backend/logs/doc_matrix.log

# Show execution progress
grep "ğŸ”„ Processing batch" backend/logs/doc_matrix.log
```

## Frontend Logging

### Browser Console

All frontend logs appear in the browser's Developer Console:
- **Chrome/Edge**: F12 â†’ Console tab
- **Firefox**: F12 â†’ Console tab
- **Safari**: Cmd+Option+C

### Frontend Log Messages

#### API Calls (`useApi.js`)
```
ğŸ“¤ API Request: POST /api/projects/Q4%20Analysis/execute
ğŸ“¥ API Response: /api/projects/Q4%20Analysis/execute (5234ms)
ğŸ’¥ API Failed: /api/projects/... (1523ms) Error: ...
```

#### Full Execution (`App.jsx`)
```
ğŸ¬ Execute All Started
  Project: Q4 Analysis
  Model: gpt-5.2
  Mode: parallel
ğŸ“¡ Sending execute request to API...
âœ… Execution complete: {ok: true, ...}
ğŸ“Š Cells completed: 15
Total Execution Time: 45234.52ms
```

#### Cell Refresh
```
ğŸ”„ Refreshing cell: document1.pdf:col_1
ğŸ“¡ Sending API request for cell document1.pdf:col_1...
âœ… Cell document1.pdf:col_1 complete
Cell document1.pdf:col_1: 5234.12ms
```

#### Chat Messages
```
ğŸ’¬ Chat Message
  Question: What are the key findings?
ğŸ“¡ Sending chat request...
âœ… Chat response received: The key findings include...
Chat Response Time: 8234.45ms
```

### Performance Timers

Frontend uses `console.time()` / `console.timeEnd()` for precise timing:
- **Total Execution Time**: Full matrix execution
- **Cell [key]**: Individual cell execution
- **Chat Response Time**: Chat query roundtrip

## Troubleshooting with Logs

### "Is it stuck or still processing?"

**Backend**: Look for these patterns:
```
ğŸš€ Starting LLM request...    â† Request sent
  ğŸ“¡ Attempt 1/3...            â† Waiting on OpenRouter
âœ… LLM response received...    â† Complete
```

If you see the ğŸš€ but no âœ… after 30+ seconds, likely waiting on LLM.

**Frontend**: Check the console.time entries:
```
Total Execution Time: <timer running>  â† Still in progress
```

### Detecting Errors vs. Slow Responses

**Errors** show:
- Backend: `âŒ` or `ğŸ’¥` symbols
- Frontend: `ğŸ’¥ API Failed` messages
- HTTP status codes (400, 500, etc.)

**Slow but working** shows:
- Steady `ğŸ“¡ Attempt N/3` messages
- No error symbols
- Timer still running

### Rate Limiting

Look for:
```
  âŒ HTTP error 429: ...
  â³ Rate limited! Waiting 2.0s before retry...
```

This is normal - the system will retry automatically.

### Finding What's Taking Long

**Backend** - grep for timing:
```bash
grep "time=" backend/logs/doc_matrix.log | sort -t= -k2 -n
```

**Frontend** - Console shows timing for each operation automatically.

## Adjusting Log Levels

### Backend

Edit `backend/main.py`:
```python
setup_logging(log_level="DEBUG")  # More verbose
setup_logging(log_level="INFO")   # Default
setup_logging(log_level="WARNING") # Less verbose
```

### Frontend

Frontend logs are always active. To reduce noise:
- Use browser console filters (e.g., filter by "API" or "LLM")
- Comment out specific console.log lines in `App.jsx` or `useApi.js`

## Log Symbols Reference

| Symbol | Meaning |
|--------|---------|
| ğŸš€ | Starting an operation |
| âœ… | Successfully completed |
| âŒ | Error occurred |
| ğŸ’¥ | Critical failure |
| ğŸ“¡ | Network request in progress |
| ğŸ“¤ | Outgoing request |
| ğŸ“¥ | Incoming response |
| ğŸ”„ | Retrying or processing |
| â³ | Waiting/delayed |
| ğŸ’¬ | Chat operation |
| ğŸ¬ | Execution starting |
| ğŸ“Š | Data/statistics |
| ğŸ“ | File/folder operation |
| ğŸ¯ | Target/goal reached |
| ğŸ”¹ | Individual item |
| ğŸ“„ | Document/row operation |

## Examples

### Normal Execution Flow
```
Backend:
â–¶ï¸  Starting full execution for project: Q4 Analysis
ğŸ“Š Execution plan: 3 documents Ã— 2 questions = 6 cells
ğŸ”„ Processing batch 1/1: 6 cells (6/6 total)
ğŸš€ Starting LLM request: model=gpt-5.2, ...
âœ… LLM response received: model=gpt-5.2, tokens=856, time=4.23s
âœ“ Cell completed: doc1.pdf Ã— col_1
... (more cells) ...
ğŸ¯ Generating summaries...
âœ… Execution complete for Q4 Analysis: 6/6 cells, 0 errors

Frontend:
ğŸ¬ Execute All Started
ğŸ“¤ API Request: POST /api/projects/...
ğŸ“¥ API Response: /api/projects/... (28450ms)
âœ… Execution complete
ğŸ“Š Cells completed: 6
Total Execution Time: 28450.23ms
```

### Error Scenario
```
Backend:
ğŸš€ Starting LLM request: model=gpt-5.2, ...
  ğŸ“¡ Attempt 1/3 - Sending request...
  âŒ Request error: Connection timeout
  ğŸ”„ Retrying in 1.0s...
  ğŸ“¡ Attempt 2/3 - Sending request...
  âŒ Request error: Connection timeout
  ğŸ”„ Retrying in 2.0s...
  ğŸ“¡ Attempt 3/3 - Sending request...
  âŒ Request error: Connection timeout
ğŸ’¥ All 3 attempts failed after 8.12s: Connection timeout
âŒ Cell failed (doc1.pdf Ã— ...): Connection timeout

Frontend:
ğŸ’¥ API Failed: /api/projects/... (8500ms)
âŒ Execution failed: Error: API error: 500 ...
```

## Best Practices

1. **Keep backend logs open** in a terminal while developing
2. **Keep browser console open** to see real-time frontend activity
3. **Use grep** to filter backend logs for specific operations
4. **Check timing** - if operations take >30s, likely waiting on LLM
5. **Look for patterns** - repeated errors vs one-off issues
6. **Save log files** when reporting bugs

---

**Quick Check**: When something seems slow:
1. Check backend terminal for `ğŸš€ Starting LLM request`
2. Wait for `âœ… LLM response received`
3. Note the `time=X.XXs` value
4. If >20s, it's LLM latency not a bug

